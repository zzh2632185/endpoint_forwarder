package proxy

import (
	"context"
	"fmt"
	"log/slog"
	"math"
	"net/http"
	"strings"
	"time"

	"endpoint_forwarder/config"
	"endpoint_forwarder/internal/endpoint"
)

// RetryHandler handles retry logic with exponential backoff
type RetryHandler struct {
	config          *config.Config
	endpointManager *endpoint.Manager
	monitoringMiddleware interface{
		RecordRetry(connID string, endpoint string)
	}
}

// NewRetryHandler creates a new retry handler
func NewRetryHandler(cfg *config.Config) *RetryHandler {
	return &RetryHandler{
		config: cfg,
	}
}

// SetEndpointManager sets the endpoint manager
func (rh *RetryHandler) SetEndpointManager(manager *endpoint.Manager) {
	rh.endpointManager = manager
}

// SetMonitoringMiddleware sets the monitoring middleware
func (rh *RetryHandler) SetMonitoringMiddleware(mm interface{
	RecordRetry(connID string, endpoint string)
}) {
	rh.monitoringMiddleware = mm
}

// Operation represents a function that can be retried, returns response and error
type Operation func(ep *endpoint.Endpoint, connID string) (*http.Response, error)

// RetryableError represents an error that can be retried with additional context
type RetryableError struct {
	Err        error
	StatusCode int
	IsRetryable bool
	Reason     string
}

func (re *RetryableError) Error() string {
	if re.Err != nil {
		return re.Err.Error()
	}
	return fmt.Sprintf("HTTP %d", re.StatusCode)
}

// Execute executes an operation with retry and fallback logic
func (rh *RetryHandler) Execute(operation Operation, connID string) (*http.Response, error) {
	return rh.ExecuteWithContext(context.Background(), operation, connID)
}

// ExecuteWithContext executes an operation with context, retry and fallback logic with group management
func (rh *RetryHandler) ExecuteWithContext(ctx context.Context, operation Operation, connID string) (*http.Response, error) {
	// Get healthy endpoints with real-time testing if enabled
	var endpoints []*endpoint.Endpoint
	if rh.endpointManager.GetConfig().Strategy.Type == "fastest" && rh.endpointManager.GetConfig().Strategy.FastTestEnabled {
		endpoints = rh.endpointManager.GetFastestEndpointsWithRealTimeTest(ctx)
	} else {
		endpoints = rh.endpointManager.GetHealthyEndpoints()
	}
	
	if len(endpoints) == 0 {
		return nil, fmt.Errorf("no healthy endpoints available in active groups")
	}

	var lastErr error
	var lastResp *http.Response
	
	// Group endpoints by group name for failure tracking
	groupEndpoints := make(map[string][]*endpoint.Endpoint)
	for _, ep := range endpoints {
		groupName := ep.Config.Group
		if groupName == "" {
			groupName = "Default"
		}
		groupEndpoints[groupName] = append(groupEndpoints[groupName], ep)
	}
	
	// Track failed groups
	failedGroups := make(map[string]bool)
	
	// Try each endpoint
	for endpointIndex, ep := range endpoints {
		// Add endpoint info to context for logging
		ctxWithEndpoint := context.WithValue(ctx, "selected_endpoint", ep.Config.Name)
		
		groupName := ep.Config.Group
		if groupName == "" {
			groupName = "Default"
		}
		
		slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("ğŸ¯ [è¯·æ±‚è½¬å‘] é€‰æ‹©ç«¯ç‚¹: %s (ç»„: %s, å°è¯• %d/%d)", 
			ep.Config.Name, groupName, endpointIndex+1, len(endpoints)))
		
		// Retry logic for current endpoint
		for attempt := 1; attempt <= rh.config.Retry.MaxAttempts; attempt++ {
			select {
			case <-ctx.Done():
				if lastResp != nil {
					lastResp.Body.Close()
				}
				return nil, ctx.Err()
			default:
			}

			// Execute operation
			resp, err := operation(ep, connID)
			if err == nil && resp != nil {
				// Check if response status code indicates success or should be retried
				retryDecision := rh.shouldRetryStatusCode(resp.StatusCode)
				
				if !retryDecision.IsRetryable {
					// Success or non-retryable error - return the response
					if attempt > 1 || endpointIndex > 0 {
						slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("âœ… [è¯·æ±‚æˆåŠŸ] ç«¯ç‚¹: %s (ç»„: %s), çŠ¶æ€ç : %d (é‡è¯• %dæ¬¡åæˆåŠŸ)", 
							ep.Config.Name, groupName, resp.StatusCode, attempt-1))
					} else {
						slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("âœ… [è¯·æ±‚æˆåŠŸ] ç«¯ç‚¹: %s (ç»„: %s), çŠ¶æ€ç : %d", 
							ep.Config.Name, groupName, resp.StatusCode))
					}
					return resp, nil
				}
				
				// Status code indicates we should retry
				slog.WarnContext(ctxWithEndpoint, fmt.Sprintf("ğŸ”„ [éœ€è¦é‡è¯•] ç«¯ç‚¹: %s (ç»„: %s, å°è¯• %d/%d) - çŠ¶æ€ç : %d (%s)", 
					ep.Config.Name, groupName, attempt, rh.config.Retry.MaxAttempts, resp.StatusCode, retryDecision.Reason))
				
				// Close the response body before retrying
				resp.Body.Close()
				lastErr = &RetryableError{
					StatusCode: resp.StatusCode,
					IsRetryable: true,
					Reason: retryDecision.Reason,
				}
			} else {
				// Network error or other failure
				lastErr = err
				if err != nil {
					slog.WarnContext(ctxWithEndpoint, fmt.Sprintf("âŒ [ç½‘ç»œé”™è¯¯] ç«¯ç‚¹: %s (ç»„: %s, å°è¯• %d/%d) - é”™è¯¯: %s", 
						ep.Config.Name, groupName, attempt, rh.config.Retry.MaxAttempts, err.Error()))
				}
			}

			// Don't wait after the last attempt on the last endpoint
			if attempt == rh.config.Retry.MaxAttempts {
				break
			}

			// Record retry (we're about to retry)
			if rh.monitoringMiddleware != nil && connID != "" {
				rh.monitoringMiddleware.RecordRetry(connID, ep.Config.Name)
			}

			// Calculate delay with exponential backoff
			delay := rh.calculateDelay(attempt)
			
			slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("â³ [ç­‰å¾…é‡è¯•] ç«¯ç‚¹: %s (ç»„: %s) - %såè¿›è¡Œç¬¬%dæ¬¡å°è¯•", 
				ep.Config.Name, groupName, delay.String(), attempt+1))

			// Wait before retry
			select {
			case <-ctx.Done():
				if lastResp != nil {
					lastResp.Body.Close()
				}
				return nil, ctx.Err()
			case <-time.After(delay):
				// Continue to next attempt
			}
		}

		slog.ErrorContext(ctxWithEndpoint, fmt.Sprintf("ğŸ’¥ [ç«¯ç‚¹å¤±è´¥] ç«¯ç‚¹ %s (ç»„: %s) æ‰€æœ‰ %d æ¬¡å°è¯•å‡å¤±è´¥", 
			ep.Config.Name, groupName, rh.config.Retry.MaxAttempts))

		// Mark endpoint's group as failed
		failedGroups[groupName] = true
		
		// Check if all endpoints in this group have been tried and failed
		groupEndpointsCount := len(groupEndpoints[groupName])
		failedEndpointsInGroup := 0
		for _, groupEp := range groupEndpoints[groupName] {
			// Count endpoints in this group that we've already tried
			for i := 0; i <= endpointIndex; i++ {
				if endpoints[i].Config.Name == groupEp.Config.Name {
					failedEndpointsInGroup++
					break
				}
			}
		}
		
		// If all endpoints in current group have failed, put group in cooldown
		if failedEndpointsInGroup == groupEndpointsCount {
			slog.WarnContext(ctxWithEndpoint, fmt.Sprintf("â„ï¸ [ç»„å¤±è´¥] ç»„ %s ä¸­æ‰€æœ‰ç«¯ç‚¹å‡å·²å¤±è´¥ï¼Œå°†ç»„è®¾ç½®ä¸ºå†·å´çŠ¶æ€", groupName))
			rh.endpointManager.GetGroupManager().SetGroupCooldown(groupName)
		}

		// If this isn't the last endpoint, log fallback
		if endpointIndex < len(endpoints)-1 {
			nextGroupName := endpoints[endpointIndex+1].Config.Group
			if nextGroupName == "" {
				nextGroupName = "Default"
			}
			slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("ğŸ”„ [åˆ‡æ¢ç«¯ç‚¹] ä» %s (ç»„: %s) åˆ‡æ¢åˆ° %s (ç»„: %s)", 
				ep.Config.Name, groupName, endpoints[endpointIndex+1].Config.Name, nextGroupName))
		}
	}

	slog.ErrorContext(ctx, fmt.Sprintf("ğŸ’¥ [å…¨éƒ¨å¤±è´¥] æ´»è·ƒç»„ä¸­æ‰€æœ‰ %d ä¸ªç«¯ç‚¹å‡ä¸å¯ç”¨ - æœ€åé”™è¯¯: %v", 
		len(endpoints), lastErr))
	return nil, fmt.Errorf("all endpoints in active groups failed after retries, last error: %w", lastErr)
}

// calculateDelay calculates the delay for exponential backoff
func (rh *RetryHandler) calculateDelay(attempt int) time.Duration {
	// Calculate exponential backoff: base_delay * (multiplier ^ (attempt - 1))
	multiplier := math.Pow(rh.config.Retry.Multiplier, float64(attempt-1))
	delay := time.Duration(float64(rh.config.Retry.BaseDelay) * multiplier)
	
	// Cap at maximum delay
	if delay > rh.config.Retry.MaxDelay {
		delay = rh.config.Retry.MaxDelay
	}
	
	return delay
}

// shouldRetryStatusCode determines if an HTTP status code should trigger a retry
func (rh *RetryHandler) shouldRetryStatusCode(statusCode int) *RetryableError {
	switch {
	case statusCode >= 200 && statusCode < 400:
		// 2xx Success and 3xx Redirects - don't retry
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "è¯·æ±‚æˆåŠŸ",
		}
	case statusCode == 400:
		// 400 Bad Request - should retry (could be temporary issue)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "è¯·æ±‚æ ¼å¼é”™è¯¯",
		}
	case statusCode == 401:
		// 401 Unauthorized - don't retry (auth issue)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "èº«ä»½éªŒè¯å¤±è´¥ï¼Œä¸é‡è¯•",
		}
	case statusCode == 403:
		// 403 Forbidden - don't retry (permission issue)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "æƒé™ä¸è¶³ï¼Œä¸é‡è¯•",
		}
	case statusCode == 404:
		// 404 Not Found - don't retry (resource doesn't exist)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "èµ„æºä¸å­˜åœ¨ï¼Œä¸é‡è¯•",
		}
	case statusCode == 429:
		// 429 Too Many Requests - should retry
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "è¯·æ±‚é¢‘ç‡è¿‡é«˜",
		}
	case statusCode >= 400 && statusCode < 500:
		// Other 4xx Client Errors - don't retry by default
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "å®¢æˆ·ç«¯é”™è¯¯ï¼Œä¸é‡è¯•",
		}
	case statusCode >= 500 && statusCode < 600:
		// 5xx Server Errors - should retry
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "æœåŠ¡å™¨é”™è¯¯",
		}
	default:
		// Unknown status code - don't retry by default
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "æœªçŸ¥çŠ¶æ€ç ",
		}
	}
}

// IsRetryableError determines if an error should trigger a retry
func (rh *RetryHandler) IsRetryableError(err error) bool {
	if err == nil {
		return false
	}

	// Handle RetryableError type
	if retryErr, ok := err.(*RetryableError); ok {
		return retryErr.IsRetryable
	}

	// Add logic to determine which errors are retryable
	// For now, we retry all errors except context cancellation
	if err == context.Canceled || err == context.DeadlineExceeded {
		return false
	}

	// Network errors, timeout errors etc. should be retried
	errorStr := strings.ToLower(err.Error())
	if strings.Contains(errorStr, "timeout") ||
		strings.Contains(errorStr, "connection refused") ||
		strings.Contains(errorStr, "connection reset") ||
		strings.Contains(errorStr, "no such host") ||
		strings.Contains(errorStr, "network unreachable") {
		return true
	}

	return true
}

// UpdateConfig updates the retry handler configuration
func (rh *RetryHandler) UpdateConfig(cfg *config.Config) {
	rh.config = cfg
}